import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from indicnlp.normalize.indic_normalize import IndicNormalizerFactory
from indicnlp.tokenize.indic_tokenize import trivial_tokenize

# Load data
df = pd.read_csv("C:/marathi-formalizer/data/marathi_formalization_dataset.csv")

# Informal-to-formal sentence dictionary
marathi_dictionary = {
    "‡§ï‡§ß‡•Ä ‡§Ø‡•á‡§£‡§æ‡§∞?": "‡§Ü‡§™‡§£ ‡§ï‡§ß‡•Ä ‡§Ø‡•á‡§£‡§æ‡§∞ ‡§Ü‡§π‡§æ‡§§?",
    "‡§ï‡§æ‡§Ø ‡§ù‡§æ‡§≤‡§Ç ‡§∞‡•á ‡§§‡•Å‡§≤‡§æ?": "‡§§‡•Å‡§≤‡§æ ‡§ï‡§æ‡§Ø ‡§ù‡§æ‡§≤‡•á ‡§Ü‡§π‡•á?",
    "‡§ï‡§æ‡§Ø ‡§ù‡§æ‡§≤‡§Ç?": "‡§Ü‡§™‡§≤‡•ç‡§Ø‡§æ‡§≤‡§æ ‡§ï‡§æ‡§Ø ‡§Ö‡§°‡§ö‡§£ ‡§Ü‡§≤‡•Ä ‡§Ü‡§π‡•á?",
    "‡§ï‡§æ‡§Ø ‡§ö‡§æ‡§≤‡§≤‡§Ç‡§Ø?": "‡§∏‡§ß‡•ç‡§Ø‡§æ ‡§ï‡§æ‡§Ø ‡§∏‡•Å‡§∞‡•Å ‡§Ü‡§π‡•á?",
    "‡§ï‡§æ‡§Ø ‡§∞‡•á ‡§π‡§æ‡§ö ‡§ï‡§æ?": "‡§π‡•Ä‡§ö ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡•Ä ‡§Ü‡§π‡•á ‡§ï‡§æ?",
    "‡§ï‡§æ‡§Ø ‡§∞‡•á ‡§Æ‡§ú‡§æ!": "‡§π‡•á ‡§ñ‡•Ç‡§™ ‡§Ü‡§®‡§Ç‡§¶‡§¶‡§æ‡§Ø‡§ï ‡§Ü‡§π‡•á!",
    "‡§•‡§æ‡§Ç‡§¨ ‡§ú‡§∞‡§æ!": "‡§ï‡•É‡§™‡§Ø‡§æ ‡§•‡•ã‡§°‡§æ ‡§µ‡•á‡§≥ ‡§•‡§æ‡§Ç‡§¨‡§æ.",
    "‡§•‡•ã‡§°‡§Ç ‡§•‡§æ‡§Ç‡§¨.": "‡§ï‡•É‡§™‡§Ø‡§æ ‡§•‡•ã‡§°‡§æ ‡§µ‡•á‡§≥ ‡§•‡§æ‡§Ç‡§¨‡§æ.",
    "‡§§‡•Å ‡§ï‡§æ‡§Ø ‡§ï‡§∞‡§§‡•ã‡§∏?": "‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§ï‡§æ‡§Ø ‡§ï‡§∞‡§§ ‡§Ü‡§π‡§æ‡§§?",
    "‡§§‡•Å‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§ ‡§Ü‡§π‡•á ‡§ï‡§æ?": "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§Ü‡§π‡•á ‡§ï‡§æ?",
    "‡§§‡•Ç ‡§ï‡•Å‡§†‡•á ‡§ú‡§æ‡§§‡•ã‡§Ø?": "‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§ï‡•Å‡§†‡•á ‡§ú‡§æ‡§§ ‡§Ü‡§π‡§æ‡§§?",
    "‡§§‡•Ç ‡§†‡•Ä‡§ï ‡§Ü‡§π‡•á‡§∏ ‡§ï‡§æ?": "‡§§‡•Å‡§Æ‡§ö‡•Ä ‡§§‡§¨‡•ç‡§Ø‡•á‡§§ ‡§†‡•Ä‡§ï ‡§Ü‡§π‡•á ‡§ï‡§æ?",
    "‡§®‡§ï‡•ã ‡§®‡§æ ‡§Ö‡§∏‡§Ç ‡§ï‡§∞‡•Ç‡§∏.": "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§∏‡•á ‡§ï‡§∞‡•Ç ‡§®‡§ï‡§æ.",
    "‡§™‡§æ‡§π‡§ø‡§≤‡§Ç ‡§ï‡§æ?": "‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§§‡•á ‡§™‡§æ‡§π‡§ø‡§≤‡§Ç ‡§ï‡§æ?",
    "‡§¨‡§ò‡§ø‡§§‡§≤‡§Ç‡§∏ ‡§ï‡§æ?": "‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§§‡•á ‡§™‡§æ‡§π‡§ø‡§≤‡§Ç ‡§ï‡§æ?",
    "‡§≠‡•á‡§ü‡•Ç ‡§®‡§Ç‡§§‡§∞.": "‡§Ü‡§™‡§£ ‡§®‡§Ç‡§§‡§∞ ‡§≠‡•á‡§ü‡•Ç‡§Ø‡§æ.",
    "‡§Æ‡§æ‡§ù‡§Ç ‡§ï‡§æ‡§Æ ‡§ù‡§æ‡§≤‡§Ç.": "‡§Æ‡§æ‡§ù‡§Ç ‡§ï‡§æ‡§Æ ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§ù‡§æ‡§≤‡§Ç ‡§Ü‡§π‡•á.",
    "‡§Æ‡§∏‡•ç‡§§ ‡§ö‡§æ‡§≤‡§≤‡§Ç‡§Ø!": "‡§∏‡§∞‡•ç‡§µ ‡§ï‡§æ‡§π‡•Ä ‡§∏‡•Å‡§∞‡§≥‡•Ä‡§§ ‡§∏‡•Å‡§∞‡•Ç ‡§Ü‡§π‡•á.",
    "‡§Æ‡§æ‡§ù‡•Ä ‡§ö‡•Ç‡§ï ‡§ù‡§æ‡§≤‡•Ä.": "‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ‡§ï‡§°‡•Ç‡§® ‡§ö‡•Ç‡§ï ‡§ù‡§æ‡§≤‡•Ä ‡§Ü‡§π‡•á.",
    "‡§Æ‡§æ‡§´ ‡§ï‡§∞.": "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§≤‡§æ ‡§ï‡•ç‡§∑‡§Æ‡§æ ‡§ï‡§∞‡§æ.",
    "‡§Æ‡§æ‡§´ ‡§ï‡§∞ ‡§®‡§æ!": "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§≤‡§æ ‡§ï‡•ç‡§∑‡§Æ‡§æ ‡§ï‡§∞‡§æ.",
    "‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ‡§ï‡§°‡§Ç ‡§™‡•à‡§∏‡•á ‡§®‡§æ‡§π‡•Ä‡§§.": "‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ‡§ï‡§°‡•á ‡§™‡•à‡§∏‡•á ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§æ‡§π‡•Ä‡§§.",
    "‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§•‡§æ‡§Ç‡§¨.": "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§•‡§æ‡§Ç‡§¨‡§æ.",
    "‡§Æ‡§≤‡§æ ‡§ù‡•ã‡§™ ‡§Ø‡•á‡§§‡•á‡§Ø.": "‡§Æ‡§≤‡§æ ‡§ù‡•ã‡§™ ‡§Ø‡•á‡§§ ‡§Ü‡§π‡•á.",
    "‡§Æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§®‡§æ‡§π‡•Ä.": "‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä‡§®‡•Å‡§∏‡§æ‡§∞ ‡§§‡•á ‡§Æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡•Ä‡§§ ‡§®‡§æ‡§π‡•Ä.",
    "‡§Æ‡§≤‡§æ ‡§∏‡§Æ‡§ú‡§≤‡§Ç ‡§®‡§æ‡§π‡•Ä.": "‡§Æ‡§≤‡§æ ‡§∏‡§Æ‡§ú‡§≤‡•á ‡§®‡§æ‡§π‡•Ä.",
    "‡§Æ‡§≤‡§æ ‡§ï‡§æ‡§π‡•Ä ‡§µ‡§ø‡§ö‡§æ‡§∞‡§æ‡§Ø‡§ö‡§Ç ‡§Ü‡§π‡•á.": "‡§Æ‡§≤‡§æ ‡§ï‡§æ‡§π‡•Ä ‡§µ‡§ø‡§ö‡§æ‡§∞‡§æ‡§Ø‡§ö‡•á ‡§Ü‡§π‡•á.",
    "‡§Æ‡§≤‡§æ ‡§µ‡•á‡§≥ ‡§®‡§æ‡§π‡•Ä.": "‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ‡§ï‡§°‡•á ‡§µ‡•á‡§≥ ‡§®‡§æ‡§π‡•Ä.",
    "‡§Æ‡§≤‡§æ ‡§™‡§æ‡§π‡§ø‡§ú‡•á ‡§§‡•á‡§ö ‡§¶‡•á.": "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§≤‡§æ ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§µ‡§∏‡•ç‡§§‡•Ç ‡§¶‡•ç‡§Ø‡§æ.",
    "‡§Æ‡§≤‡§æ ‡§≠‡•á‡§ü‡§æ‡§Ø‡§ö‡§Ç ‡§Ü‡§π‡•á.": "‡§Æ‡§≤‡§æ ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§≠‡•á‡§ü‡§æ‡§Ø‡§ö‡•á ‡§Ü‡§π‡•á.",
    "‡§Æ‡§≤‡§æ ‡§µ‡§æ‡§ü‡§§‡§Ç ‡§§‡§∏‡§Ç ‡§®‡§æ‡§π‡•Ä.": "‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ ‡§Æ‡§§‡•á ‡§§‡§∏‡•á ‡§®‡§æ‡§π‡•Ä.",
    "‡§Æ‡§≤‡§æ ‡§π‡§µ‡§Ø‡§Ç!": "‡§Æ‡§≤‡§æ ‡§§‡•á ‡§π‡§µ‡•á ‡§Ü‡§π‡•á.",
    "‡§Æ‡§æ‡§´ ‡§ï‡§∞‡§∂‡•Ä‡§≤ ‡§ï‡§æ?": "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§≤‡§æ ‡§ï‡•ç‡§∑‡§Æ‡§æ ‡§ï‡§∞‡§æ‡§≤ ‡§ï‡§æ?",
    "‡§∏‡§æ‡§Ç‡§ó ‡§®‡§æ ‡§Æ‡§≤‡§æ!": "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§≤‡§æ ‡§∏‡§æ‡§Ç‡§ó‡§æ.",
    "‡§∏‡•Å‡§ü‡•ç‡§ü‡•Ä ‡§Ü‡§π‡•á ‡§ï‡§æ?": "‡§Ü‡§ú ‡§∏‡•Å‡§ü‡•ç‡§ü‡•Ä ‡§Ü‡§π‡•á ‡§ï‡§æ?",
    "‡§π‡•á ‡§ñ‡§∞‡§Ç ‡§Ü‡§π‡•á ‡§ï‡§æ?": "‡§π‡•á ‡§∏‡§§‡•ç‡§Ø ‡§Ü‡§π‡•á ‡§ï‡§æ?",
    "‡§π‡•á ‡§ò‡•á.": "‡§ï‡•É‡§™‡§Ø‡§æ ‡§π‡•á ‡§ò‡•ç‡§Ø‡§æ.",
    "‡§π‡•á ‡§Æ‡§∏‡•ç‡§§ ‡§Ü‡§π‡•á!": "‡§π‡•á ‡§Ö‡§§‡§ø‡§∂‡§Ø ‡§â‡§§‡•ç‡§ï‡•É‡§∑‡•ç‡§ü ‡§Ü‡§π‡•á!",
    "‡§π‡§æ ‡§≠‡§æ‡§∞‡•Ä ‡§Ü‡§π‡•á!": "‡§π‡§æ ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§ö‡§æ‡§Ç‡§ó‡§≤‡§æ ‡§Ü‡§π‡•á!",
    "‡§π‡•ã ‡§ï‡§æ?": "‡§ñ‡§∞‡•ã‡§ñ‡§∞ ‡§Ö‡§∏‡•á ‡§Ü‡§π‡•á ‡§ï‡§æ?",
    "‡§π‡•ã ‡§Æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§ ‡§Ü‡§π‡•á.": "‡§π‡•ã‡§Ø, ‡§Æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§Ü‡§π‡•á.",
    "‡§π‡•ã ‡§π‡•ã!": "‡§π‡•ã‡§Ø.",
    "‡§Ö‡§ú‡•Ç‡§® ‡§µ‡•á‡§≥ ‡§≤‡§æ‡§ó‡•á‡§≤ ‡§ï‡§æ?": "‡§•‡•ã‡§°‡§æ ‡§Ö‡§ß‡§ø‡§ï ‡§µ‡•á‡§≥ ‡§≤‡§æ‡§ó‡•á‡§≤ ‡§ï‡§æ?",
    "‡§Ü‡§™‡§£ ‡§≠‡•á‡§ü‡§≤‡•ã ‡§ï‡§æ ‡§™‡•Ç‡§∞‡•ç‡§µ‡•Ä?": "‡§Ü‡§™‡§£ ‡§™‡•Ç‡§∞‡•ç‡§µ‡•Ä ‡§ï‡§ß‡•Ä ‡§≠‡•á‡§ü‡§≤‡•ã ‡§Ü‡§π‡•ã‡§§ ‡§ï‡§æ?",
    "‡§ö‡§≤ ‡§≠‡•á‡§ü‡•Ç ‡§®‡§Ç‡§§‡§∞.": "‡§Ü‡§™‡§£ ‡§®‡§Ç‡§§‡§∞ ‡§≠‡•á‡§ü‡•Ç‡§Ø‡§æ.",
    "‡§ö‡§≤ ‡§®‡§ø‡§ò‡•Ç‡§Ø‡§æ.": "‡§ö‡§≤‡§æ ‡§Ü‡§™‡§£ ‡§®‡§ø‡§ò‡•Ç‡§Ø‡§æ."
}

# Synonym dictionary
synonym_dict = {
    "‡§∏‡§æ‡§Ç‡§ó": "‡§ï‡•É‡§™‡§Ø‡§æ",
    "‡§®‡§æ": "‡§Æ‡§≤‡§æ",
    "‡§Æ‡§≤‡§æ": "‡§∏‡§æ‡§Ç‡§ó‡§æ",
    "‡§Æ‡§≤‡§æ!": "‡§∏‡§æ‡§Ç‡§ó‡§æ.",
    "‡§Æ‡§≤‡§æ!!": "‡§∏‡§æ‡§Ç‡§ó‡§æ.",
    "‡§ö‡§≤": "‡§Ü‡§™‡§£",
    "‡§≠‡•á‡§ü‡•Ç": "‡§®‡§Ç‡§§‡§∞",
    "‡§®‡§Ç‡§§‡§∞": "‡§≠‡•á‡§ü‡•Ç‡§Ø‡§æ",
    "‡§®‡§Ç‡§§‡§∞.": "‡§≠‡•á‡§ü‡•Ç‡§Ø‡§æ.",
    "‡§ï‡§æ‡§Ø": "‡§§‡•Å‡§≤‡§æ",
    "‡§ï‡§æ?!": "‡§ï‡§æ?",
    "‡§§‡•Å‡§≤‡§æ": "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ",
    "‡§§‡•Å‡§≤‡§æ?": "‡§Ü‡§π‡•á?",
    "‡§§‡•Å‡§≤‡§æ?!": "‡§Ü‡§π‡•á?",
    "‡§∞‡•á": "‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡•Ä",
    "‡§π‡§æ‡§ö": "‡§Ü‡§π‡•á",
    "‡§ñ‡§∞‡§Ç": "‡§∏‡§§‡•ç‡§Ø",
    "‡§ù‡§æ‡§≤‡§Ç": "‡§ï‡§æ‡§Ø",
    "‡§ù‡§æ‡§≤": "‡§ï‡§æ‡§Ø",
    "‡§ù‡§æ‡§≤‡§Ç‡§Ø": "‡§ï‡§æ‡§Ø",
    "‡§ù‡§æ‡§≤‡•á‡§≤‡•Ä": "‡§ï‡§æ‡§Ø",
    "‡§ù‡§æ‡§≤‡•Ä": "‡§ï‡§æ‡§Ø",
    "‡§Æ‡§æ‡§π‡§ø‡§§": "‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä",
    "‡§π‡§æ‡§ö": "‡§Ü‡§π‡•á"
}

def normalize_synonyms(text):
    words = text.split()
    normalized = [synonym_dict.get(word, word) for word in words]
    return ' '.join(normalized)

# Simple stemmer
def stem_word(word):
    suffixes = ['‡§§ ‡§Ü‡§π‡•á', '‡§≤‡•Ä ‡§Ü‡§π‡•á', '‡§≤‡•á ‡§Ü‡§π‡•á', '‡§≤‡•á‡§≤‡§æ', '‡§≤‡•á‡§≤‡•Ä', '‡§≤‡•ã', '‡§≤‡§æ', '‡§≤‡•Ä', '‡§§', '‡§≤‡•á', '‡§æ‡§§', '‡§æ', '‡•Ä', '‡•á', '‡•ã']
    for suffix in suffixes:
        if word.endswith(suffix):
            return word[:-len(suffix)]
    return word

# Indic NLP
factory = IndicNormalizerFactory()
normalizer = factory.get_normalizer("mr")

def marathi_lemmatize(text):
    text = normalizer.normalize(text)
    return ' '.join(trivial_tokenize(text, lang="mr"))

# Preprocess pipeline

def preprocess_sentence(sentence):
    if sentence in marathi_dictionary:
        sentence = marathi_dictionary[sentence]
    sentence = re.sub(r'[^\w\s]', '', sentence)
    sentence = sentence.strip().lower()
    sentence = marathi_lemmatize(sentence)
    sentence = normalize_synonyms(sentence)
    words = sentence.split()
    new_words = [stem_word(word) for word in words]
    return " ".join(new_words)

# Apply preprocessing
df["input_processed"] = df["informal"].apply(preprocess_sentence)
df["target_processed"] = df["formal"].apply(preprocess_sentence)

# Split data
X_train, X_test, y_train, y_test = train_test_split(df["input_processed"], df["target_processed"], test_size=0.2, random_state=42)

# ML model
model = Pipeline([
    ("tfidf", TfidfVectorizer(ngram_range=(1, 2))),
    ("classifier", LogisticRegression(max_iter=200))
])

# Train
model.fit(X_train, y_train)

# Predict
preds = model.predict(X_test)

# Evaluate
print("\nüßæ Classification Report:")
print(classification_report(y_test, preds))

# Test function
def formalize(text):
    cleaned = preprocess_sentence(text)
    return model.predict([cleaned])[0]

# Try test
print("\nüîÅ Formalized:", formalize("‡§§‡•Å ‡§ï‡§æ‡§Ø ‡§ï‡§∞‡§§‡•ã‡§∏?"))
